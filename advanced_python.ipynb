{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gstreamer Pipelines \n",
    "## von der Kamera zum DNN mit Python \n",
    "\n",
    "In diesem Jupyter Notebook findet ihr die Ausarbeitung zur Vorlesung im Wahlpflichtkurs \"Advanced Python\" an der Beuth Hochschule für Technik. Die Umsetzung ähnlicher Projekte sollhiermit erleichtert werden. \n",
    "\n",
    "Jeder Rechner mit Ubuntu 18.04 sollte die gleichen Befehle benötigen. NVIDIA AGX Xavier besitzt eine aarch64 Architektur. Der Guide funktioniert somit auch für Arm basierte Systeme. \n",
    "\n",
    "Am Ende des Notebooks versteht ihr den Umgang mit Gstreamer und könnt eine dedizierte Video-Pipeline die durch ein DNN klassifiziert wird erstellen. \n",
    "\n",
    "Im verkürzten \"Coronasemester\" SS20 habe ich leider keine weitere Energie bereit gehabt um dieses Projekt vollständiger abzuschließen. Möchte jedoch die ursprünglich geplante Bildkorrektur von einer Aufnahme im Dämmerlicht zu einer im Tageslicht in den kommenden Wochen zu Ende ausarbeiten. \n",
    "\n",
    "Die Gliederung\n",
    "1. Einrichtung der Hardware\n",
    "2. Grundlagen von Gstreamer\n",
    "3. Verbindung zu OpenCV und Einbindung des DNN\n",
    "4. dynamische Pipelines mit GstInterpipe & GstInference (von RidgeRun)\n",
    "\n",
    "<img src=\"pics/day-night.jpg\" style=\"height:1000px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Einrichtung der Hardware\n",
    "### 1.1 - Einrichtung der SSH-Verbindung\n",
    "\n",
    "Ich nutzen den Raspberrry Pi 2 als Bridge, um preiswerte Sensoren nutzen zu können. Der Anschluss geschieht via Cross-Kabel und SSH.\n",
    "\n",
    "Rasbian und Raspberry OS sollte bereits vorkonfiguriert sein. Folgende Einstellungen sind im Konfigurationsmenü zu aktivieren.\n",
    "1. Der SSH-Server\n",
    "2. X11-forwarding (Fensterteilung über ssh)\n",
    "3. Der CSI Kameraport\n",
    "4. Um die Leistung zu verbessern, kann man den Pi in die CLI booten.\n",
    "5. Die automatische Anmeldung kann gewählt werden.\n",
    "6. Der Default-Name des Pi ist \"pi\", kann aber angepasst werden\n",
    "\n",
    "Für jedes genutzte Gerät muss eine eindeutige IP4 Adresse fest vergeben werden. Ich nutze als neues Subnetz 10.0.0.1 für den Receiver und 10.0.0.5 für den Raspberry Pi. DHCP muss abgestellt sein. Moderne Ubuntus bieten in den Netzwerkeinstellungen die Option \"Direktverbindung\". In älteren Distros nennt es sich \"Ad-Hoc\" Verbindung.\n",
    "\n",
    "Nun kann man sich mit dem Befehl \"ssh -X pi@<chosen_ip4_address>\" als Remote-Terminal, mit Fenster-Weitergabe anmelden.\n",
    "\n",
    "### 1.2 - Einrichtung des Jetson AGX Xavier mit NVIDIAS Developer Kit\n",
    "\n",
    "Hierfür benötigt man bei Ersteinrichtung ein zweites Gerät mit Ubuntu 18.04 . Die Installation setzt eine aktuelle Python Umgebung voraus. Man nutzt das kostenfreie NVIDIA Developer Kit zur Installation. Da die Software-Pakete sich noch stark verändern, sollte man von umfassender Personalisierung absehen, man wird das System oft neu aufsetzen.\n",
    "\n",
    "### 1.3 - Einrichtung der Kamera\n",
    "Um die unterstützten Auflösungen und Codierung der Kamera aufzulisten kann der Befehl\n",
    "\n",
    "_v4l2-ctl --list-formats-ext_\n",
    "\n",
    "verwendet werden. Weitere Befehle sind (X mit 0-63 ersetzen)\n",
    "\n",
    "_v4l2-compliance -d /dev/videoX -f_ \n",
    "\n",
    "_v4l2-ctl -d /dev/videoX --all_\n",
    "\n",
    "### 1.4 - Vergrößern des UDP-Buffer\n",
    "Damit mehrere Streams über den gleichen Socket versendet werden können, muss die UDP-Puffer vergrößert werden.\n",
    "Dazu kann man sich die aktuelle Größe mit den Befehlen\n",
    "\n",
    "_sysctl net.core.rmem_max_\n",
    "\n",
    "_sysctl net.core.rmem_default_\n",
    "\n",
    "ausgeben lassen. Und mit den Befehlen\n",
    "\n",
    "\n",
    "sudo sysctl -w net.core.rmem_max=8388608\n",
    "\n",
    "sudo sysctl -w net.core.wmem_max=8388608\n",
    "\n",
    "auf 8MB erweitern.\n",
    "\n",
    "### 1.5 Gstreamer installieren\n",
    "GStreamer ist in der Standardinstallation von Ubuntu enthalten, kann ansonsten aber über das folgende Paket installiert werden:\n",
    "\n",
    "_sudo apt-get install libgstreamer1.0-0_\n",
    "\n",
    "\n",
    "Empfehlenswert ist weiterhin noch das Paket:\n",
    "\n",
    "\n",
    "_sudo apt-get install gstreamer1.0-tools_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Grundlagen von Gstreamer\n",
    "\n",
    "Gstreamer (https://gstreamer.freedesktop.org/) ist Open Source. In Gstreamer bildet man komplexe Pipelines zur  Erstellung von \"Streaming Media Apllication\".\n",
    "\n",
    "Eine Gstreamer Pipeline ist modular aufgebaut. Sie wird aus den vielen unterschiedlichen PlugIns zusammen gehängt.\n",
    "Gstreamer kann in vielen verschiedenen Programmiersprachen bearbeitet werden. \n",
    "https://gstreamer.freedesktop.org/bindings/\n",
    "\n",
    "(Der Standardbaustein eines Muxer oder Demuxer ist im unteren Bild nicht abgebildet.)\n",
    "\n",
    "<img src=\"pics/thread-buffering.png\" style=\"width:1000px;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Kamerastream vom RaspberryPi senden\n",
    "- Auch die CSI Kamera im Raspberry wird über die Video4Linux2 Treiber angesprochen. \n",
    "- Die Kameras liegen dabei im Verzeichnes /dev mit der Bezeichnung video0 bis video63\n",
    "- Videoformat sowie Chroma (Interlaced YUV 4:2:0) sind Mindestangaben!\n",
    "- um den Videostrem im \"Real Time Streaming Protocol\" zu versenden wird die _rtpvrawpay_ Node genutzt.\n",
    "- die udpsink wandelt den Payload in UPD-konforme Pakete um.\n",
    "- Wir möchten keinen Sync, das heißt Pakete die nicht verarbeitet werden können werden verworfen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "gst-launch-1.0 -ev v4l2src \\\n",
    "device=/dev/video0 !      \\\n",
    "'video/x-raw, width=128, height=96,' \\\n",
    "'format=I420, framerate=10/1'!       \\\n",
    "rtpvrawpay !              \\\n",
    "udpsink host=10.0.0.1 port=5000      \\\n",
    "sync=false async=false\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - Kamerastream empfangen\n",
    "Auf der Empfängerseite muss der Stream in der umgekehrten Reihenfolge mit den entsprechenden Modulen umgewandelt werden.\n",
    "Als End-Node wird \"xvimagesink\" genutzt. die ein X-Fenster öffnet um das Video abzuspielen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "gst-launch-1.0 udpsrc port=5000 !\n",
    "'application/x-rtp, media=(string)video,'\n",
    "'encoding-name=(string)RAW, sampling=YCbCr-4:2:0,'\n",
    "'width=(string)128, height=(string)96' !\n",
    "rtpvrawdepay ! xvimagesink\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Informationen der Shell, bei setzen des -ev flag können ebenfalls nötig sein. Zum Beispiel braucht man den timestamp-offset bei der Synchronisation und dem scrollen durch Streams. Die Informationen jedes Moduls werden hierbei aufgeführt.\n",
    "<img src=\"pics/pi_gstreamer.png\" style=\"width:1000px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Gstreamer zu OpenCV\n",
    "### 3.1 - Gst-Python\n",
    "Für das aufsetzen des Videostream bietet sich Python an. Hierbei kann man einen Factory Pattern nutzen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gi\n",
    "from gi.repository import Gst, GObjekt\n",
    "GObject.threads_init\n",
    "Gst.init(None)\n",
    "\n",
    "class SimplePipeline(object):\n",
    "    def __init__(self):\n",
    "        self.pipeline = Gst.Pipeline()\n",
    "        self.filesrc = Gst.ElementFactory.make('filesrc')\n",
    "        self.oggdemux = Gst.ElementFactory.make('oggdemux')\n",
    "\n",
    "        self.pipeline.add(self.filesrc)         # Module dem Bauplan zufügen\n",
    "        self.pipeline.add(self.oggdemux)\n",
    "        self.pipeline.add(self.theoradec)\n",
    "        self.pipeline.add(self.videoconvert)\n",
    "        self.pipeline.add(self.xvimagesink)\n",
    "\n",
    "        self.filesrc.link(self.oggdemux)        # Module verlinken\n",
    "        self.oggdemux.link(self.theoracodec)\n",
    "        self.theoracodec.link(self.videoconvert)\n",
    "        self.videoconvert.link(self.xvimagesink)\n",
    "\n",
    "loop = GObject.MainLoop()\n",
    "\n",
    "p = SimplePipeline()\n",
    "\n",
    "p.filesrc.set_property('location', video 'video.ogv')\n",
    "p.pipeline.set_state(Gst.State.PLAYING)\n",
    "loop.run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oder, je nachdem, was man übersichtlicher findet auch string-parsing. Das Debugging größerer Pipelines kann sonst umständlich werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplePipeline(object)\n",
    "    def __init__(self):\n",
    "        pipe_desc = ('filesrc name=src !'\n",
    "                    'oggdemux ! theoradec !'\n",
    "                    'videoconvert ! xvimagesink')\n",
    "        self.pipeline = Gst.parse_launch(pipe_desc)\n",
    "        self-filesrc = self.pipeline.get_by_name('src')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - OpenCV um Videostreams zu nutzen\n",
    "OpenCV ist zurecht der Standard für Computer Vision. Es beinhaltet über 2500 Module mit absolut aktuellem Bezug.\n",
    "https://docs.opencv.org/master/d9/df8/tutorial_root.html\n",
    "Wir nutzen es um die Gstreamer Pipeline zu bearbeiten. Den Videostream anzupassen und durch ein DNN zu bewerten. Die Ergebnisse werden ebenfalls mithilfe von OpenCV herausgestellt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import der benötigten Pakete\n",
    "from __future__ import print_function\n",
    "from imutils.video import WebcamVideoStream # https://github.com/jrosebr1/imutils/ für Threads\n",
    "import cv2                                  # https://github.com/opencv/opencv\n",
    "import subprocess\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "from queue import Queue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Als erstes laden wir die trainierten Gewichte, und die Erkennungsklassen in OpenCV.\n",
    "Wir sehen hier im Modellnamen, dass Bilder zur Verarbeitung die Maße 300px x 300 px haben müssen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# die Erkkennungslabel müssen angelegt werden. (z.B. 1: 'random' etc.)\n",
    "classNames = { 0: 'face' }            \n",
    "    \n",
    "# Wir laden das Caffe oder Tensorflow Modell\n",
    "net = cv2.dnn.readNetFromCaffe('resnet10/deploy.prototxt', 'resnet10/res10_300x300_ssd_iter_140000.caffemodel')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 - Gstreamer Pipeline definieren\n",
    "\n",
    "Die Initialisierung der Pipeline benötigt, wie oben besprochen weitere Module. Um den Videostream einzulesen werden zwei weitere Konvertierungen, in das Format _NV12_ und von dort nach _RGB_ vorgenommen. Weitere Informationen hierzu:\n",
    "https://gstreamer.freedesktop.org/documentation/additional/design/mediatype-video-raw.html?gi-language=c#page-description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def receive():\n",
    "    gst_str = ('udpsrc port=5000 caps = \"application/x-rtp, media=(string)video, clock-rate=(int)90000, '\n",
    "               'encoding-name=(string)RAW, sampling=YCbCr-4:2:0,depth=(string)8,colorimetry=(string)BT601-5, '\n",
    "               'width=(string)720, height=(string)576, payload=(int)96, a-framerate=10\" ! '\n",
    "               'rtpvrawdepay ! videoconvert ! '\n",
    "               'video/x-raw, format=(string)NV12! '\n",
    "               'videoconvert ! appsink emit-signals=True')\n",
    "    return  WebcamVideoStream(gst_str, cv2.CAP_GSTREAMER).start()# mit Threading\n",
    "    #return cv2.VideoCapture(gst_str, cv2.CAP_GSTREAMER)  # ohne Threading\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir passen das zu öffnende Fenster ein wenig an."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_NAME = 'CameraDemo'\n",
    "    \n",
    "def open_window():\n",
    "    cv2.startWindowThread()\n",
    "    cv2.namedWindow(WINDOW_NAME, cv2.WINDOW_NORMAL)\n",
    "    cv2.resizeWindow(WINDOW_NAME, 720, 576)\n",
    "    cv2.moveWindow(WINDOW_NAME, 0, 0)\n",
    "    cv2.setWindowTitle(WINDOW_NAME, 'RTSP-Camera Demo for Xavier AGX')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um Das Video mit dem DNN zu untersuchen muss es in eine (Bild)Datei mit 300px x 300px umgewandelt werden\n",
    "https://www.pyimagesearch.com/2017/11/06/deep-learning-opencvs-blobfromimage-works/\n",
    "Ergebnisse mit mehr als 20% Korrelation werden durch ein Rechteck markiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def read_cam(cap):\n",
    "    while True:\n",
    "       \n",
    "        img = cap.read() # Folgebild von Kamera lesen\n",
    "        frame = img\n",
    "\n",
    "        # Abmessungen auslesen und in einen \"Blob\" umwandeln\n",
    "        (h, w) = frame.shape[:2]\n",
    "        blob = cv2.dnn.blobFromImage(cv2.resize(frame, (300, 300)), 1.0,\n",
    "            (300, 300), (104.0, 177.0, 123.0))\n",
    "\n",
    "        # Den Blob durchs DNN analysieren lassen und die Vorhersagen in detections \n",
    "        # abspeichern\n",
    "        net.setInput(blob)\n",
    "        detections = net.forward()\n",
    "        \n",
    "        for i in range(0, detections.shape[2]):\n",
    "            # Die Wahrscheinlichkeit der Vorhersagen iterativ abfragen.\n",
    "            confidence = detections[0, 0, i, 2]\n",
    "            # schwache Vorhersagen (mit einem Wert unter 20%) filtern\n",
    "            if confidence < 0.2:\n",
    "                continue\n",
    "            # (x, y)- Koordinaten zur Berechnung eines Rechtecks nutzen\n",
    "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "            (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "\n",
    "            # Umgranzungskästen zeichen und die Wahrscheinlichkeit als Text \n",
    "            # oben drüber schreiben\n",
    "            text = \"{:.2f}%\".format(confidence * 100)\n",
    "            y = startY - 10 if startY - 10 > 10 else startY + 10\n",
    "            cv2.rectangle(frame, (startX, startY), (endX, endY),\n",
    "                (0, 0, 255), 2)\n",
    "            cv2.putText(frame, text, (startX, y),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 255), 2)\n",
    "        # Fenster öffnen \n",
    "        cv2.imshow(WINDOW_NAME, frame)\n",
    "\n",
    "def main():\n",
    "    # Verbindung öffnen\n",
    "    cap = receive()\n",
    "    # Fenster öffnen\n",
    "    open_window()\n",
    "    # Kamerastream einlesen und analysieren\n",
    "    read_cam(cap)\n",
    "    \n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In der Funktion `recieve()` wurde eine OpenCV Funktion ersetzt. Der zugehörige Code steht im folgenden Feld. Hiermit wird ein eigener Thread für den Videostream geöffnet. Bei der Arbeit mit mehreren Videostreams ist diese Ersetzung essentiell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for threaded videostreams by Adrian Rosebrock\n",
    "from threading import Thread\n",
    "import cv2\n",
    "class WebcamVideoStream:\n",
    "    def __init__(self, src=0):\n",
    "        # initialize the video camera stream and read the first frame\n",
    "        # from the stream\n",
    "        self.stream = cv2.VideoCapture(src)\n",
    "        (self.grabbed, self.frame) = self.stream.read()\n",
    "        # initialize the variable used to indicate if the thread should\n",
    "        # be stopped\n",
    "        self.stopped = False\n",
    "        \n",
    "    def start(self):\n",
    "        # start the thread to read frames from the video stream\n",
    "        Thread(target=self.update, args=()).start()\n",
    "        return self\n",
    "    def update(self):\n",
    "    #keep looping infinitely until the thread is stopped\n",
    "        while True:\n",
    "            # if the thread indicator variable is set, stop the thread\n",
    "            if self.stopped:\n",
    "                return\n",
    "            # otherwise, read the next frame from the stream\n",
    "            (self.grabbed, self.frame) = self.stream.read()\n",
    "    def read(self):\n",
    "        # return the frame most recently read\n",
    "        return self.frame\n",
    "    def stop(self):\n",
    "        # indicate that the thread should be stopped\n",
    "        self.stopped = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you can use `insert_audio_clip` and `insert_ones` to create a new training example.\n",
    "\n",
    "```\n",
    "X = GRU(units = 128, return_sequences = True)(X)\n",
    "```\n",
    "\n",
    "**Exercise**: Implement `create_training_example()`. You will need to carry out the following steps:\n",
    "\n",
    "1. Initialize the label vector $y$ as a numpy array of zeros and shape $(1, T_y)$.\n",
    "2. Initialize the set of existing segments to an empty list.\n",
    "3. Randomly select 0 to 4 \"activate\" audio clips, and insert them onto the 10sec clip. Also insert labels at the correct position in the label vector $y$.\n",
    "4. Randomly select 0 to 2 negative audio clips, and insert them into the 10sec clip. \n"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "nlp-sequence-models",
   "graded_item_id": "rSupZ",
   "launcher_item_id": "cvGhe"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
